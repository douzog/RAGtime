{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d3931e",
   "metadata": {},
   "source": [
    "# Hi! \n",
    "\n",
    "Here, I explore how to build a RAG model that I can \"chat with\" my research papers and find details, and if there is time, find exactly the source where my prompt response originated form, similar to Olmo2. https://allenai.org/olmo\n",
    "\n",
    "This notebook can do the following:\n",
    "\n",
    "\n",
    "- Loads your PDFs\n",
    "- Extracts text and metadata (including source file and page)\n",
    "- Creates embeddings using Ollama (nomic-embed-text)\n",
    "- Runs a local RAG chain with Ollama LLM (e.g., llama3)\n",
    "- Returns the answer along with the source file and page number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cda88",
   "metadata": {},
   "source": [
    "# Installations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ee8aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (comment out when finished)\n",
    "# !pip install langchain-ollama langchain chromadb langchain-community pathlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698f2f1",
   "metadata": {},
   "source": [
    "# Imports!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5e987b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.document_loaders import PyMuPDFLoader \n",
    "from langchain_community.vectorstores import Chroma \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67a251",
   "metadata": {},
   "source": [
    "#  Path to your PDFs / Chroma_DB!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19fb83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_directory = './pdfs'  # Change this to your local directory with PDFs\n",
    "prompts_and_reponses_directory = './prompts_and_responses'\n",
    "\n",
    "for path in [pdf_directory, prompts_and_reponses_directory]:\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b869c9",
   "metadata": {},
   "source": [
    "# Load your PDFs!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd614325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 chunks from PDFs\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for file in Path(pdf_directory).glob(\"*.pdf\"):\n",
    "    loader = PyMuPDFLoader(str(file))\n",
    "    loaded_docs = loader.load()\n",
    "    for doc in loaded_docs:\n",
    "        doc.metadata['source'] = file.name\n",
    "    docs.extend(loaded_docs)\n",
    "\n",
    "print(f\"Loaded {len(docs)} chunks from PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba782e77",
   "metadata": {},
   "source": [
    "Now a little more technical lets split some chuks of text to overlap, feel fre to adjust numbers!\n",
    "\n",
    "Then create a chroma vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8abb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d06c8",
   "metadata": {},
   "source": [
    "## Pull the desired model on your terminal!\n",
    "get what you need: `$ ollama pull llama3 `\n",
    "\n",
    "show what you got: `$ ollama list`\n",
    "\n",
    "this is crutial before running the next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e059e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model='llama3')  # Change model if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c5edf",
   "metadata": {},
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8893f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chroma vector store and persist it to disk\n",
    "persist_directory = f\"./chroma_db_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "vectordb = Chroma.from_documents(\n",
    "    split_docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86a5a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds the text of split_docs into numerical vectors using the embedding model, and stores them in the database.\n",
    "vectordb = Chroma.from_documents(split_docs, embedding=embedding, persist_directory=persist_directory) # this one takes a while\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "302508c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG with Ollama LLM\n",
    "llm = OllamaLLM(model='llama3')\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectordb.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508ec37",
   "metadata": {},
   "source": [
    "# Get ready to Prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df96dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_rag_response_json(query, response):\n",
    "    \"\"\"\n",
    "    Saves a RAG query, its answer, and source documents to a timestamped JSON file.\n",
    "\n",
    "    Args:\n",
    "        query (str): The question asked to the QA chain.\n",
    "        response (dict): The response from LangChain's RetrievalQA with 'result' and 'source_documents'.\n",
    "        output_dir (str): Directory to save the output file. Created if it doesn't exist.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_dir = f\"./prompts_and_responses\"\n",
    "    filename = os.path.join(output_dir, f\"{timestamp}_rag_response.json\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Format the data to be saved\n",
    "    data = {\n",
    "        \"datetime\": timestamp,\n",
    "        \"response_heading\": response[\"result\"],\n",
    "        \"query\": query,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "                \"page\": doc.metadata.get(\"page\", \"?\"),\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            for doc in response[\"source_documents\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(response['result'] + \"\\n\\n\")\n",
    "    print(f\"(ﾉ◕ヮ◕)ﾉ Response saved to JSON: {filename} \\n\\n\")\n",
    "\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        print(json.dumps(data, indent=4))  # Pretty-print with 4-space indentation\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b44f56",
   "metadata": {},
   "source": [
    "Type prompts as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb51ca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of artificial intelligence model that is trained on large amounts of text data and can generate human-like language outputs.\n",
      "\n",
      "\n",
      "(ﾉ◕ヮ◕)ﾉ Response saved to JSON: ./prompts_and_responses/2025-08-17_14-20-31_rag_response.json \n",
      "\n",
      "\n",
      "{\n",
      "    \"datetime\": \"2025-08-17_14-20-31\",\n",
      "    \"response_heading\": \"A Large Language Model (LLM) is a type of artificial intelligence model that is trained on large amounts of text data and can generate human-like language outputs.\",\n",
      "    \"query\": \"what's an LLM?\",\n",
      "    \"sources\": [\n",
      "        {\n",
      "            \"source\": \"s41524-024-01423-2.pdf\",\n",
      "            \"page\": 6,\n",
      "            \"metadata\": {\n",
      "                \"format\": \"PDF 1.4\",\n",
      "                \"file_path\": \"pdfs/s41524-024-01423-2.pdf\",\n",
      "                \"trapped\": \"\",\n",
      "                \"creationDate\": \"D:20241102043739+05'30'\",\n",
      "                \"subject\": \"npj Computational Materials, doi:10.1038/s41524-024-01423-2\",\n",
      "                \"source\": \"s41524-024-01423-2.pdf\",\n",
      "                \"title\": \"Opportunities for retrieval and tool augmented large language models in scientific facilities\",\n",
      "                \"creationdate\": \"2024-11-02T04:37:39+05:30\",\n",
      "                \"producer\": \"iText\\u00ae 5.3.5 \\u00a92000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)\",\n",
      "                \"modDate\": \"D:20241105094313+01'00'\",\n",
      "                \"page\": 6,\n",
      "                \"keywords\": \"\",\n",
      "                \"creator\": \"Springer\",\n",
      "                \"author\": \"Michael H. Prince\",\n",
      "                \"moddate\": \"2024-11-05T09:43:13+01:00\",\n",
      "                \"total_pages\": 8\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"s41524-024-01423-2.pdf\",\n",
      "            \"page\": 6,\n",
      "            \"metadata\": {\n",
      "                \"page\": 6,\n",
      "                \"creator\": \"Springer\",\n",
      "                \"file_path\": \"pdfs/s41524-024-01423-2.pdf\",\n",
      "                \"trapped\": \"\",\n",
      "                \"author\": \"Michael H. Prince\",\n",
      "                \"source\": \"s41524-024-01423-2.pdf\",\n",
      "                \"creationDate\": \"D:20241102043739+05'30'\",\n",
      "                \"keywords\": \"\",\n",
      "                \"title\": \"Opportunities for retrieval and tool augmented large language models in scientific facilities\",\n",
      "                \"moddate\": \"2024-11-05T09:43:13+01:00\",\n",
      "                \"format\": \"PDF 1.4\",\n",
      "                \"modDate\": \"D:20241105094313+01'00'\",\n",
      "                \"creationdate\": \"2024-11-02T04:37:39+05:30\",\n",
      "                \"subject\": \"npj Computational Materials, doi:10.1038/s41524-024-01423-2\",\n",
      "                \"total_pages\": 8,\n",
      "                \"producer\": \"iText\\u00ae 5.3.5 \\u00a92000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"s41524-024-01423-2.pdf\",\n",
      "            \"page\": 1,\n",
      "            \"metadata\": {\n",
      "                \"title\": \"Opportunities for retrieval and tool augmented large language models in scientific facilities\",\n",
      "                \"modDate\": \"D:20241105094313+01'00'\",\n",
      "                \"subject\": \"npj Computational Materials, doi:10.1038/s41524-024-01423-2\",\n",
      "                \"format\": \"PDF 1.4\",\n",
      "                \"creationDate\": \"D:20241102043739+05'30'\",\n",
      "                \"total_pages\": 8,\n",
      "                \"author\": \"Michael H. Prince\",\n",
      "                \"source\": \"s41524-024-01423-2.pdf\",\n",
      "                \"trapped\": \"\",\n",
      "                \"producer\": \"iText\\u00ae 5.3.5 \\u00a92000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)\",\n",
      "                \"file_path\": \"pdfs/s41524-024-01423-2.pdf\",\n",
      "                \"page\": 1,\n",
      "                \"creator\": \"Springer\",\n",
      "                \"keywords\": \"\",\n",
      "                \"creationdate\": \"2024-11-02T04:37:39+05:30\",\n",
      "                \"moddate\": \"2024-11-05T09:43:13+01:00\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"s41524-024-01423-2.pdf\",\n",
      "            \"page\": 1,\n",
      "            \"metadata\": {\n",
      "                \"title\": \"Opportunities for retrieval and tool augmented large language models in scientific facilities\",\n",
      "                \"format\": \"PDF 1.4\",\n",
      "                \"file_path\": \"pdfs/s41524-024-01423-2.pdf\",\n",
      "                \"creator\": \"Springer\",\n",
      "                \"producer\": \"iText\\u00ae 5.3.5 \\u00a92000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)\",\n",
      "                \"author\": \"Michael H. Prince\",\n",
      "                \"keywords\": \"\",\n",
      "                \"creationDate\": \"D:20241102043739+05'30'\",\n",
      "                \"total_pages\": 8,\n",
      "                \"moddate\": \"2024-11-05T09:43:13+01:00\",\n",
      "                \"modDate\": \"D:20241105094313+01'00'\",\n",
      "                \"creationdate\": \"2024-11-02T04:37:39+05:30\",\n",
      "                \"trapped\": \"\",\n",
      "                \"page\": 1,\n",
      "                \"source\": \"s41524-024-01423-2.pdf\",\n",
      "                \"subject\": \"npj Computational Materials, doi:10.1038/s41524-024-01423-2\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./prompts_and_responses/2025-08-17_14-20-31_rag_response.json'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what's an LLM?\"\n",
    "response = qa_chain(query)\n",
    "save_rag_response_json(query, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd33b7a",
   "metadata": {},
   "source": [
    "# Finished!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_FCS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
