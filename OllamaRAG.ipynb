{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d3931e",
   "metadata": {},
   "source": [
    "# Hi! \n",
    "\n",
    "Here, I explore how to build a RAG model that I can \"chat with\" my research papers and find details, and if there is time, find exactly the source where my prompt response originated form, similar to Olmo2. https://allenai.org/olmo\n",
    "\n",
    "This notebook can do the following:\n",
    "\n",
    "\n",
    "- Loads your PDFs\n",
    "- Extracts text and metadata (including source file and page)\n",
    "- Creates embeddings using Ollama (nomic-embed-text)\n",
    "- Runs a local RAG chain with Ollama LLM (e.g., llama3)\n",
    "- Returns the answer along with the source file and page number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cda88",
   "metadata": {},
   "source": [
    "# Installations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ee8aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (comment out when finished)\n",
    "# !pip install langchain-ollama langchain chromadb langchain-community pathlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698f2f1",
   "metadata": {},
   "source": [
    "# Imports!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5e987b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.document_loaders import PyMuPDFLoader \n",
    "from langchain_community.vectorstores import Chroma \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67a251",
   "metadata": {},
   "source": [
    "#  Path to your PDFs / Chroma_DB!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19fb83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_directory = './pdfs'  # Change this to your local directory with PDFs\n",
    "persist_directory = './chroma_db'\n",
    "prompts_and_reponses_directory = './prompts_and_responses'\n",
    "\n",
    "for path in [pdf_directory, persist_directory, prompts_and_reponses_directory]:\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b869c9",
   "metadata": {},
   "source": [
    "# Load your PDFs!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd614325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 chunks from PDFs\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for file in Path(pdf_directory).glob(\"*.pdf\"):\n",
    "    loader = PyMuPDFLoader(str(file))\n",
    "    loaded_docs = loader.load()\n",
    "    for doc in loaded_docs:\n",
    "        doc.metadata['source'] = file.name\n",
    "    docs.extend(loaded_docs)\n",
    "\n",
    "print(f\"Loaded {len(docs)} chunks from PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba782e77",
   "metadata": {},
   "source": [
    "Now a little more technical lets split some chuks of text to overlap, feel fre to adjust numbers!\n",
    "\n",
    "Then create a chroma vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8abb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d06c8",
   "metadata": {},
   "source": [
    "#### make sure you pull tinyllama model on your terminal!\n",
    "\n",
    "1. Pull what you need\n",
    "\n",
    "- $ ollama pull tinyllama\n",
    "2. Show what you got\n",
    "\n",
    "- $ ollama list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e059e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes an embedding function, OllamaEmbeddings tells LangChain to use Ollama’s local inference server to generate vector embeddings.\n",
    "embedding = OllamaEmbeddings(model='llama3')  # Change model if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e86e7a",
   "metadata": {},
   "source": [
    "What the Chroma.from_documents(..) do?\n",
    "- split_docs: your list of short text chunks from PDFs\n",
    "- embedding=...: the function used to convert each chunk into a vector\n",
    "- persist_directory=...: the folder to store your vector database on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c5edf",
   "metadata": {},
   "source": [
    "What the vectordb.persist() funciton do? \n",
    "- Saves everything (vectors + metadata) to the persist_directory\n",
    "- This makes your RAG system persistent — so you can skip reprocessing next time you run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8893f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "vec = embedding.embed_query(\"test\")\n",
    "dim = len(vec) \n",
    "print(f\"Embedding dimension size: {dim}\")\n",
    "\n",
    "persist_directory = f\"./chroma_db_llama_{dim}_{timestamp}/\" #  directory includes dimension size and timestamp\n",
    "vectordb = Chroma.from_documents(\n",
    "    split_docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d37f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb = Chroma.from_documents(split_docs, embedding=embedding, persist_directory=persist_directory) # comment when done, this one takes a while\n",
    "# vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f271e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG chain with Ollama LLM\n",
    "llm = OllamaLLM(model='llama3')\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectordb.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b3cc0",
   "metadata": {},
   "source": [
    "# Get ready to roll! Ask questions to your pdf dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df96dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_rag_response_json(query, response):\n",
    "    \"\"\"\n",
    "    Saves a RAG query, its answer, and source documents to a timestamped JSON file.\n",
    "\n",
    "    Args:\n",
    "        query (str): The question asked to the QA chain.\n",
    "        response (dict): The response from LangChain's RetrievalQA with 'result' and 'source_documents'.\n",
    "        output_dir (str): Directory to save the output file. Created if it doesn't exist.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_dir = f\"./prompts_and_responses\"\n",
    "    filename = os.path.join(output_dir, f\"{timestamp}_rag_response.json\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Format the data to be saved\n",
    "    data = {\n",
    "        \"datetime\": timestamp,\n",
    "        \"response_heading\": response[\"result\"],\n",
    "        \"query\": query,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "                \"page\": doc.metadata.get(\"page\", \"?\"),\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            for doc in response[\"source_documents\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(response['result'] + \"\\n\\n\")\n",
    "    print(f\"Done! (ﾉ◕ヮ◕)ﾉ !! Response saved to JSON: {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508ec37",
   "metadata": {},
   "source": [
    "Type your query here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb51ca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A helpful answer!\n",
      "\n",
      "According to the provided context, a Large Language Model (LLM) is not explicitly defined. However, we can infer that it refers to a type of language model that is capable of processing and generating human-like text.\n",
      "\n",
      "From the references provided, we see that LLMs are mentioned as being used in various applications such as robotics, scientific instrumentation, and conversational AI systems. This suggests that LLMs are large-scale neural networks trained on vast amounts of text data, enabling them to understand and generate human language.\n",
      "\n",
      "In summary, while an explicit definition is not provided, we can infer from the context that a Large Language Model (LLM) is a type of AI model designed for processing and generating human-like text.\n",
      "\n",
      "\n",
      "Done! (ﾉ◕ヮ◕)ﾉ !! Response saved to JSON: ./prompts_and_responses/2025-07-27_15-14-44_rag_response.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./prompts_and_responses/2025-07-27_15-14-44_rag_response.json'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is an llm?\"\n",
    "response = qa_chain(query)\n",
    "save_rag_response_json(query, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9765a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_FCS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
